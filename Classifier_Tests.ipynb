{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd098b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Classifier Tests Jupyter Notebook"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "# Import myutils.py from mysklearn forlder\n",
    "import mysklearn.myutils\n",
    "importlib.reload(mysklearn.myutils)\n",
    "import mysklearn.myutils as myutils\n",
    "\n",
    "# Import mypytable.py from mysklearn folder\n",
    "import mysklearn.mypytable\n",
    "importlib.reload(mysklearn.mypytable)\n",
    "from mysklearn.mypytable import MyPyTable \n",
    "\n",
    "# Import myclassifiers.py from mysklearn folder\n",
    "import mysklearn.myclassifiers\n",
    "importlib.reload(mysklearn.myclassifiers)\n",
    "from mysklearn.myclassifiers import MyRandomForestClassifier, MyKNeighborsClassifier, MyDecisionTreeClassifier, MyNaiveBayesClassifier\n",
    "\n",
    "# Import myevaluation.py from mysklearn folder\n",
    "import mysklearn.myevaluation\n",
    "importlib.reload(mysklearn.myevaluation)\n",
    "import mysklearn.myevaluation as myevaluation"
   ]
  },
  {
   "source": [
    "## Random Forest Classifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code goes here"
   ]
  },
  {
   "source": [
    "## Naive Bayes Classifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-----------------------------------------------------------\nAccuracy and Error Rate\n-----------------------------------------------------------\n\nNaive Bayes: accuracy = 0.6, error rate = 0.4\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(mysklearn.mypytable)\n",
    "importlib.reload(mysklearn.myevaluation)\n",
    "import numpy as np\n",
    "\n",
    "# Get data from csv file\n",
    "table = MyPyTable().load_from_file(os.path.join(\"input_files\",\"winequality-red.csv\"))\n",
    "y_col = table.get_column(\"quality\", False)\n",
    "x_cols = table.drop_col(\"quality\")\n",
    "\n",
    "# Use Naive Bayes to classify\n",
    "testcase = MyNaiveBayesClassifier()\n",
    "\n",
    "#Returns x INDEXES\n",
    "X_train, X_test = myevaluation.stratified_kfold_cross_validation(x_cols,y_col,n_splits=10)\n",
    "X_train, X_test, y_train, y_test = myutils.getInstances(X_train, X_test, x_cols,y_col)\n",
    "\n",
    "predicted_values = []\n",
    "for i,fold in enumerate(X_train):\n",
    "    train,test = myutils.normalize_values(X_train[i],X_test[i])\n",
    "    testcase.fit(train,y_train[i])\n",
    "    predicted_values.append(testcase.predict(test))\n",
    "\n",
    "numCorrectPredictions = 0\n",
    "numWrongPredictions = 0\n",
    "for i,fold in enumerate(X_test):\n",
    "    for index in range(len(fold)):\n",
    "        values = [predicted_values[i][index], y_test[i][index]] #predicted/actual\n",
    "        if(values[0]==values[1]):\n",
    "            numCorrectPredictions = numCorrectPredictions+1\n",
    "        else:\n",
    "            numWrongPredictions = numWrongPredictions+1\n",
    "\n",
    "accuracy = np.round((numCorrectPredictions)/(numCorrectPredictions+numWrongPredictions),3)\n",
    "error_rate = np.round((numWrongPredictions)/(numCorrectPredictions+numWrongPredictions),3)\n",
    "\n",
    "print(\"-----------------------------------------------------------\")\n",
    "print(\"Accuracy and Error Rate\")\n",
    "print(\"-----------------------------------------------------------\")\n",
    "print()\n",
    "print(\"Naive Bayes: accuracy = {}, error rate = {}\".format(accuracy,error_rate))"
   ]
  },
  {
   "source": [
    "## Decision Tree Classifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ", 2.3, 0.066, 35.0, 70.0, 0.9958799999999999, 3.43, 0.62, 10.1], [6.8, 0.66, 0.07, 1.6, 0.07, 16.0, 61.0, 0.99572, 3.29, 0.6, 9.3], [7.5, 0.58, 0.03, 4.1, 0.08, 27.0, 46.0, 0.99592, 3.02, 0.47, 9.2], [7.4, 0.55, 0.19, 1.8, 0.08199999999999999, 15.0, 34.0, 0.99655, 3.49, 0.68, 10.5], [7.5, 0.58, 0.2, 2.0, 0.073, 34.0, 44.0, 0.9949399999999999, 3.1, 0.43, 9.3], [7.6, 0.79, 0.21, 2.3, 0.087, 21.0, 68.0, 0.9955, 3.12, 0.44, 9.2], [7.8, 0.87, 0.26, 3.8, 0.107, 31.0, 67.0, 0.9966799999999999, 3.26, 0.46, 9.2], [6.9, 0.57, 0.0, 2.8, 0.081, 21.0, 41.0, 0.99518, 3.41, 0.52, 10.8], [7.8, 0.5, 0.09, 2.2, 0.115, 10.0, 42.0, 0.9971, 3.18, 0.62, 9.5], [7.1, 0.755, 0.15, 1.8, 0.107, 20.0, 84.0, 0.99593, 3.19, 0.5, 9.5], [8.7, 0.675, 0.1, 1.6, 0.09, 4.0, 11.0, 0.99745, 3.31, 0.65, 9.55], [7.8, 0.53, 0.01, 1.6, 0.077, 3.0, 19.0, 0.995, 3.16, 0.46, 9.8], [6.9, 0.58, 0.2, 1.75, 0.057999999999999996, 8.0, 22.0, 0.9932200000000001, 3.38, 0.49, 11.7], [6.7, 0.7, 0.08, 3.75, 0.067, 8.0, 16.0, 0.99334, 3.43, 0.52, 12.6], [5.6, 0.54, 0.04, 1.7, 0.049, 5.0, 13.0, 0.9942, 3.72, 0.58, 11.4], [6.1, 0.32, 0.25, 2.3, 0.071, 23.0, 58.0, 0.9963299999999999, 3.42, 0.97, 10.6], [7.0, 0.57, 0.02, 2.0, 0.07200000000000001, 17.0, 26.0, 0.99575, 3.36, 0.61, 10.2], [7.8, 0.6, 0.26, 2.0, 0.08, 31.0, 131.0, 0.9962200000000001, 3.21, 0.52, 9.9], [6.2, 0.46, 0.29, 2.1, 0.07400000000000001, 32.0, 98.0, 0.9957799999999999, 3.33, 0.62, 9.8], [7.8, 0.645, 0.0, 5.5, 0.086, 5.0, 18.0, 0.9986, 3.4, 0.55, 9.6], [6.3, 0.3, 0.48, 1.8, 0.069, 18.0, 61.0, 0.9959, 3.44, 0.78, 10.3], [9.4, 0.4, 0.31, 2.2, 0.09, 13.0, 62.0, 0.9966, 3.07, 0.63, 10.5], [6.9, 0.49, 0.1, 2.3, 0.07400000000000001, 12.0, 30.0, 0.9959, 3.42, 0.58, 10.2], [6.7, 0.62, 0.21, 1.9, 0.079, 8.0, 62.0, 0.997, 3.52, 0.58, 9.3], [7.7, 0.43, 0.25, 2.6, 0.073, 29.0, 63.0, 0.99615, 3.37, 0.58, 10.5], [8.2, 1.0, 0.09, 2.3, 0.065, 7.0, 37.0, 0.99685, 3.32, 0.55, 9.0], [11.5, 0.18, 0.51, 4.0, 0.10400000000000001, 4.0, 23.0, 0.9996, 3.28, 0.97, 10.1], [6.9, 0.36, 0.25, 2.4, 0.098, 5.0, 16.0, 0.9964, 3.41, 0.6, 10.1], [9.0, 0.46, 0.31, 2.8, 0.09300000000000001, 19.0, 98.0, 0.99815, 3.32, 0.63, 9.5], [8.0, 0.58, 0.28, 3.2, 0.066, 21.0, 114.0, 0.9973, 3.22, 0.54, 9.4], [9.1, 0.785, 0.0, 2.6, 0.09300000000000001, 11.0, 28.0, 0.9994, 3.36, 0.86, 9.4], [14.0, 0.41, 0.63, 3.8, 0.08900000000000001, 6.0, 47.0, 1.0014, 3.01, 0.81, 10.8], [8.3, 0.66, 0.15, 1.9, 0.079, 17.0, 42.0, 0.9972, 3.31, 0.54, 9.6], [10.4, 0.34, 0.58, 3.7, 0.174, 6.0, 16.0, 0.997, 3.19, 0.7, 11.3], [11.1, 0.45, 0.73, 3.2, 0.066, 6.0, 22.0, 0.9986, 3.17, 0.66, 11.2], [11.5, 0.315, 0.54, 2.1, 0.084, 5.0, 15.0, 0.9987, 2.98, 0.7, 9.2], [10.2, 0.645, 0.36, 1.8, 0.053, 5.0, 14.0, 0.9982, 3.17, 0.42, 10.0], [10.0, 0.59, 0.31, 2.2, 0.09, 26.0, 62.0, 0.9994, 3.18, 0.63, 10.2], [9.1, 0.22, 0.24, 2.1, 0.078, 1.0, 28.0, 0.9990000000000001, 3.41, 0.87, 10.3], [9.4, 0.43, 0.24, 2.8, 0.092, 14.0, 45.0, 0.998, 3.19, 0.73, 10.0], [6.2, 0.36, 0.24, 2.2, 0.095, 19.0, 42.0, 0.9946, 3.57, 0.57, 11.7], [8.5, 0.585, 0.18, 2.1, 0.078, 5.0, 30.0, 0.9967, 3.2, 0.48, 9.8], [9.2, 0.755, 0.18, 2.2, 0.14800000000000002, 10.0, 103.0, 0.9969, 2.87, 1.36, 10.2], [7.2, 0.52, 0.07, 1.4, 0.07400000000000001, 5.0, 20.0, 0.9973, 3.32, 0.81, 9.6], [7.0, 0.65, 0.02, 2.1, 0.066, 8.0, 25.0, 0.9972, 3.47, 0.67, 9.5], [6.4, 0.865, 0.03, 3.2, 0.071, 27.0, 58.0, 0.995, 3.61, 0.49, 12.7], [8.8, 0.7, 0.0, 1.7, 0.069, 8.0, 19.0, 0.9970100000000001, 3.31, 0.53, 10.0], [10.0, 0.56, 0.24, 2.2, 0.079, 19.0, 58.0, 0.9991, 3.18, 0.56, 10.1], [12.9, 0.5, 0.55, 2.8, 0.07200000000000001, 7.0, 24.0, 1.0001200000000001, 3.09, 0.68, 10.9], [9.3, 0.36, 0.39, 1.5, 0.08, 41.0, 55.0, 0.9965200000000001, 3.47, 0.73, 10.9], [8.8, 0.61, 0.19, 4.0, 0.094, 30.0, 69.0, 0.99787, 3.22, 0.5, 10.0], [6.1, 0.56, 0.0, 2.2, 0.079, 6.0, 9.0, 0.9948, 3.59, 0.54, 11.5], [8.4, 0.62, 0.12, 1.8, 0.07200000000000001, 38.0, 46.0, 0.9950399999999999, 3.38, 0.89, 11.8], [9.5, 0.37, 0.52, 2.0, 0.08800000000000001, 12.0, 51.0, 0.99613, 3.29, 0.58, 11.1], [9.1, 0.5, 0.3, 1.9, 0.065, 8.0, 17.0, 0.99774, 3.32, 0.71, 10.5], [10.5, 0.39, 0.46, 2.2, 0.075, 14.0, 27.0, 0.99598, 3.06, 0.84, 11.4], [8.9, 0.745, 0.18, 2.5, 0.077, 15.0, 48.0, 0.99739, 3.2, 0.47, 9.7], [8.2, 0.64, 0.27, 2.0, 0.095, 5.0, 77.0, 0.9974700000000001, 3.13, 0.62, 9.1], [7.2, 0.38, 0.3, 1.8, 0.073, 31.0, 70.0, 0.99685, 3.42, 0.59, 9.5], [6.1, 0.48, 0.09, 1.7, 0.078, 18.0, 30.0, 0.9940200000000001, 3.45, 0.54, 11.2], [7.0, 0.69, 0.07, 2.5, 0.091, 15.0, 21.0, 0.99572, 3.38, 0.6, 11.3], [8.0, 0.25, 0.43, 1.7, 0.067, 22.0, 50.0, 0.9946, 3.38, 0.6, 11.9], [7.8, 0.39, 0.42, 2.0, 0.086, 9.0, 21.0, 0.99526, 3.39, 0.66, 11.6], [7.1, 0.59, 0.0, 2.2, 0.078, 26.0, 44.0, 0.9952200000000001, 3.42, 0.68, 10.8], [9.1, 0.4, 0.57, 4.6, 0.08, 6.0, 20.0, 0.9965200000000001, 3.28, 0.57, 12.5], [9.6, 0.38, 0.42, 1.9, 0.071, 5.0, 13.0, 0.99659, 3.15, 0.75, 10.5], [10.5, 0.36, 0.47, 2.2, 0.07400000000000001, 9.0, 23.0, 0.9963799999999999, 3.23, 0.76, 12.0], [7.1, 0.6, 0.01, 2.3, 0.079, 24.0, 37.0, 0.9951399999999999, 3.4, 0.61, 10.9], [7.3, 0.44, 0.2, 1.6, 0.049, 24.0, 64.0, 0.9935, 3.38, 0.57, 11.7], [8.2, 0.635, 0.1, 2.1, 0.073, 25.0, 60.0, 0.9963799999999999, 3.29, 0.75, 10.9], [7.0, 0.36, 0.21, 2.4, 0.086, 24.0, 69.0, 0.99556, 3.4, 0.53, 10.1], [6.7, 0.46, 0.24, 1.7, 0.077, 18.0, 34.0, 0.9948, 3.39, 0.6, 10.6], [8.2, 0.44, 0.24, 2.3, 0.063, 10.0, 28.0, 0.99613, 3.25, 0.53, 10.2], [7.2, 0.45, 0.15, 2.0, 0.078, 10.0, 28.0, 0.9960899999999999, 3.29, 0.51, 9.9], [6.0, 0.51, 0.0, 2.1, 0.064, 40.0, 54.0, 0.995, 3.54, 0.93, 10.7], [10.2, 0.54, 0.37, 15.4, 0.214, 55.0, 95.0, 1.00369, 3.18, 0.77, 9.0], [6.8, 0.64, 0.03, 2.3, 0.075, 14.0, 31.0, 0.99545, 3.36, 0.58, 10.4], [7.5, 0.38, 0.57, 2.3, 0.106, 5.0, 12.0, 0.99605, 3.36, 0.55, 11.4], [6.5, 0.53, 0.06, 2.0, 0.063, 29.0, 44.0, 0.9948899999999999, 3.38, 0.83, 10.3], [5.4, 0.58, 0.08, 1.9, 0.059000000000000004, 20.0, 31.0, 0.99484, 3.5, 0.64, 10.2], [6.2, 0.51, 0.14, 1.9, 0.055999999999999994, 15.0, 34.0, 0.9939600000000001, 3.48, 0.57, 11.5], [7.2, 0.39, 0.44, 2.6, 0.066, 22.0, 48.0, 0.9949399999999999, 3.3, 0.84, 11.5], [6.0, 0.31, 0.47, 3.6, 0.067, 18.0, 42.0, 0.99549, 3.39, 0.66, 11.0], [9.6, 0.32, 0.47, 1.4, 0.055999999999999994, 9.0, 24.0, 0.99695, 3.22, 0.82, 10.3], [7.7, 0.27, 0.68, 3.5, 0.358, 5.0, 10.0, 0.9972, 3.25, 1.08, 9.9], [6.6, 0.815, 0.02, 2.7, 0.07200000000000001, 17.0, 34.0, 0.9955, 3.58, 0.89, 12.3], [12.0, 0.39, 0.66, 3.0, 0.09300000000000001, 12.0, 30.0, 0.9996, 3.18, 0.63, 10.8], [10.4, 0.33, 0.63, 2.8, 0.084, 5.0, 22.0, 0.9998, 3.26, 0.74, 11.2], [10.4, 0.24, 0.46, 1.8, 0.075, 6.0, 21.0, 0.9976, 3.25, 1.02, 10.8], [7.7, 0.915, 0.12, 2.2, 0.14300000000000002, 7.0, 23.0, 0.9964, 3.35, 0.65, 10.2], [7.5, 0.27, 0.34, 2.3, 0.05, 4.0, 8.0, 0.9951, 3.4, 0.64, 11.0], [8.8, 0.31, 0.4, 2.8, 0.109, 7.0, 16.0, 0.9961399999999999, 3.31, 0.79, 11.8], [8.7, 0.33, 0.38, 3.3, 0.063, 10.0, 19.0, 0.9946799999999999, 3.3, 0.73, 12.0], [8.9, 0.12, 0.45, 1.8, 0.075, 10.0, 21.0, 0.9955200000000001, 3.41, 0.76, 11.9], [7.0, 0.4, 0.32, 3.6, 0.061, 9.0, 29.0, 0.99416, 3.28, 0.49, 11.3], [9.1, 0.3, 0.34, 2.0, 0.064, 12.0, 25.0, 0.99516, 3.26, 0.84, 11.7], [8.7, 0.41, 0.41, 6.2, 0.078, 25.0, 42.0, 0.9953, 3.24, 0.77, 12.6], [9.1, 0.25, 0.34, 2.0, 0.071, 45.0, 67.0, 0.99769, 3.44, 0.86, 10.2], [8.8, 0.24, 0.35, 1.7, 0.055, 13.0, 27.0, 0.9939399999999999, 3.14, 0.59, 11.3], [8.2, 0.33, 0.39, 2.5, 0.07400000000000001, 29.0, 48.0, 0.9952799999999999, 3.32, 0.88, 12.4], [9.8, 0.3, 0.39, 1.7, 0.062, 3.0, 9.0, 0.9948, 3.14, 0.57, 11.5], [6.6, 0.58, 0.02, 2.0, 0.062, 37.0, 53.0, 0.99374, 3.35, 0.76, 11.6], [7.0, 0.56, 0.17, 1.7, 0.065, 15.0, 24.0, 0.9951399999999999, 3.44, 0.68, 10.55], [9.2, 0.52, 1.0, 3.4, 0.61, 32.0, 69.0, 0.9996, 2.74, 2.0, 9.4], [9.9, 0.5, 0.24, 2.3, 0.10300000000000001, 6.0, 14.0, 0.9978, 3.34, 0.52, 10.0], [11.6, 0.47, 0.44, 1.6, 0.147, 36.0, 51.0, 0.99836, 3.38, 0.86, 9.9], [8.1, 0.73, 0.0, 2.5, 0.081, 12.0, 24.0, 0.99798, 3.38, 0.46, 9.6], [6.2, 0.785, 0.0, 2.1, 0.06, 6.0, 13.0, 0.99664, 3.59, 0.61, 10.0], [10.7, 0.35, 0.53, 2.6, 0.07, 5.0, 16.0, 0.9972, 3.15, 0.65, 11.0], [7.4, 0.36, 0.3, 1.8, 0.07400000000000001, 17.0, 24.0, 0.99419, 3.24, 0.7, 11.4], [7.3, 0.98, 0.05, 2.1, 0.061, 20.0, 49.0, 0.99705, 3.31, 0.55, 9.7], [7.5, 0.5, 0.36, 6.1, 0.071, 17.0, 102.0, 0.9978, 3.35, 0.8, 10.5], [7.6, 0.41, 0.24, 1.8, 0.08, 4.0, 11.0, 0.9962, 3.28, 0.59, 9.5], [7.7, 0.935, 0.43, 2.2, 0.114, 22.0, 114.0, 0.997, 3.25, 0.73, 9.2], [8.8, 0.4, 0.4, 2.2, 0.079, 19.0, 52.0, 0.998, 3.44, 0.64, 9.2], [9.7, 0.32, 0.54, 2.5, 0.094, 28.0, 83.0, 0.9984, 3.28, 0.82, 9.6], [9.3, 0.39, 0.44, 2.1, 0.107, 34.0, 125.0, 0.9978, 3.14, 1.22, 9.5], [8.1, 0.575, 0.22, 2.1, 0.077, 12.0, 65.0, 0.9967, 3.29, 0.51, 9.2], [8.0, 0.71, 0.0, 2.6, 0.08, 11.0, 34.0, 0.9976, 3.44, 0.53, 9.5], [8.3, 0.715, 0.15, 1.8, 0.08900000000000001, 10.0, 52.0, 0.9968, 3.23, 0.77, 9.5], [7.5, 0.6, 0.03, 1.8, 0.095, 25.0, 99.0, 0.995, 3.35, 0.54, 10.1], [7.8, 0.63, 0.48, 1.7, 0.1, 14.0, 96.0, 0.9961, 3.19, 0.62, 9.5], [7.2, 0.73, 0.02, 2.5, 0.076, 16.0, 42.0, 0.9972, 3.44, 0.52, 9.3], [7.6, 0.55, 0.21, 2.2, 0.071, 7.0, 28.0, 0.9964, 3.28, 0.55, 9.7], [8.7, 0.625, 0.16, 2.0, 0.10099999999999999, 13.0, 49.0, 0.9962, 3.14, 0.57, 11.0], [7.1, 0.68, 0.07, 1.9, 0.075, 16.0, 51.0, 0.99685, 3.38, 0.52, 9.5], [7.9, 0.37, 0.23, 1.8, 0.077, 23.0, 49.0, 0.9963, 3.28, 0.67, 9.3], [10.8, 0.5, 0.46, 2.5, 0.073, 5.0, 27.0, 1.0001, 3.05, 0.64, 9.5], [7.4, 0.36, 0.29, 2.6, 0.087, 26.0, 72.0, 0.99645, 3.39, 0.68, 11.0], [13.5, 0.53, 0.79, 4.8, 0.12, 23.0, 77.0, 1.0018, 3.18, 0.77, 13.0], [9.5, 0.37, 0.52, 2.0, 0.08199999999999999, 6.0, 26.0, 0.998, 3.18, 0.51, 9.5], [8.6, 0.725, 0.24, 6.6, 0.11699999999999999, 31.0, 134.0, 1.0014, 3.32, 1.07, 9.3], [12.5, 0.38, 0.6, 2.6, 0.081, 31.0, 72.0, 0.9996, 3.1, 0.73, 10.5], [13.0, 0.32, 0.65, 2.6, 0.09300000000000001, 15.0, 47.0, 0.9996, 3.05, 0.61, 10.6], [7.2, 0.34, 0.32, 2.5, 0.09, 43.0, 113.0, 0.9966, 3.32, 0.79, 11.1], [9.9, 0.63, 0.24, 2.4, 0.077, 6.0, 33.0, 0.9974, 3.09, 0.57, 9.4], [15.5, 0.645, 0.49, 4.2, 0.095, 10.0, 23.0, 1.00315, 2.92, 0.74, 11.1], [12.3, 0.5, 0.49, 2.2, 0.08900000000000001, 5.0, 14.0, 1.0002, 3.19, 0.44, 9.6], [13.2, 0.38, 0.55, 2.7, 0.081, 5.0, 16.0, 1.0006, 2.98, 0.54, 9.4], [6.8, 0.69, 0.0, 5.6, 0.124, 21.0, 58.0, 0.9997, 3.46, 0.72, 10.2], [9.5, 0.59, 0.44, 2.3, 0.071, 21.0, 68.0, 0.9992, 3.46, 0.63, 9.5], [10.7, 0.43, 0.39, 2.2, 0.106, 8.0, 32.0, 0.9986, 2.89, 0.5, 9.6], [8.6, 0.8, 0.11, 2.3, 0.084, 12.0, 31.0, 0.9979, 3.4, 0.48, 9.9], [7.7, 0.66, 0.04, 1.6, 0.039, 4.0, 9.0, 0.9962, 3.4, 0.47, 9.4], [10.6, 1.025, 0.43, 2.8, 0.08, 21.0, 84.0, 0.9985, 3.06, 0.57, 10.1], [8.9, 0.48, 0.24, 2.85, 0.094, 35.0, 106.0, 0.9982, 3.1, 0.53, 9.2], [7.7, 0.56, 0.2, 2.0, 0.075, 9.0, 39.0, 0.9987, 3.48, 0.62, 9.3], [8.3, 0.65, 0.1, 2.9, 0.08900000000000001, 17.0, 40.0, 0.99803, 3.29, 0.55, 9.5], [8.8, 0.59, 0.18, 2.9, 0.08900000000000001, 12.0, 74.0, 0.9973799999999999, 3.14, 0.54, 9.4], [6.5, 0.46, 0.14, 2.4, 0.114, 9.0, 37.0, 0.9973200000000001, 3.66, 0.65, 9.8], [8.6, 0.55, 0.09, 3.3, 0.068, 8.0, 17.0, 0.99735, 3.23, 0.44, 10.0], [7.1, 0.48, 0.28, 2.8, 0.068, 6.0, 16.0, 0.99682, 3.24, 0.53, 10.3], [6.4, 0.64, 0.21, 1.8, 0.081, 14.0, 31.0, 0.9968899999999999, 3.59, 0.66, 9.8], [6.9, 0.56, 0.03, 1.5, 0.086, 36.0, 46.0, 0.9952200000000001, 3.53, 0.57, 10.6], [9.2, 0.58, 0.2, 3.0, 0.081, 15.0, 115.0, 0.998, 3.23, 0.59, 9.5], [6.2, 0.46, 0.17, 1.6, 0.073, 7.0, 11.0, 0.99425, 3.61, 0.54, 11.4], [8.4, 0.59, 0.29, 2.6, 0.109, 31.0, 119.0, 0.9980100000000001, 3.15, 0.5, 9.1], [8.2, 0.43, 0.29, 1.6, 0.081, 27.0, 45.0, 0.99603, 3.25, 0.54, 10.3], [7.1, 0.43, 0.17, 1.8, 0.08199999999999999, 27.0, 51.0, 0.99634, 3.49, 0.64, 10.4], [9.4, 0.4, 0.47, 2.5, 0.087, 6.0, 20.0, 0.99772, 3.15, 0.5, 10.5], [8.3, 0.6, 0.25, 2.2, 0.11800000000000001, 9.0, 38.0, 0.99616, 3.15, 0.53, 9.8], [6.7, 0.64, 0.23, 2.1, 0.08, 11.0, 119.0, 0.9953799999999999, 3.36, 0.7, 10.9], [9.0, 0.58, 0.25, 2.0, 0.10400000000000001, 8.0, 21.0, 0.99769, 3.27, 0.72, 9.6], [7.5, 0.58, 0.14, 2.2, 0.077, 27.0, 60.0, 0.9963, 3.28, 0.59, 9.8], [7.0, 0.42, 0.19, 2.3, 0.071, 18.0, 36.0, 0.9947600000000001, 3.39, 0.56, 10.9], [7.5, 0.61, 0.26, 1.9, 0.073, 24.0, 88.0, 0.9961200000000001, 3.3, 0.53, 9.8], [9.1, 0.775, 0.22, 2.2, 0.079, 12.0, 48.0, 0.9976, 3.18, 0.51, 9.6], [9.0, 0.6, 0.29, 2.0, 0.069, 32.0, 73.0, 0.99654, 3.34, 0.57, 10.0], [7.3, 0.74, 0.08, 1.7, 0.094, 10.0, 45.0, 0.9957600000000001, 3.24, 0.5, 9.8], [8.0, 0.81, 0.25, 3.4, 0.076, 34.0, 85.0, 0.9966799999999999, 3.19, 0.42, 9.2], [7.3, 0.59, 0.26, 2.0, 0.08, 17.0, 104.0, 0.99584, 3.28, 0.52, 9.9], [7.5, 0.4, 0.18, 1.6, 0.079, 24.0, 58.0, 0.9965, 3.34, 0.58, 9.4], [6.9, 0.63, 0.02, 1.9, 0.078, 18.0, 30.0, 0.9971200000000001, 3.4, 0.75, 9.8], [6.1, 0.6, 0.08, 1.8, 0.071, 14.0, 45.0, 0.99336, 3.38, 0.54, 11.0], [5.6, 0.54, 0.04, 1.7, 0.049, 5.0, 13.0, 0.9942, 3.72, 0.58, 11.4], [7.4, 0.47, 0.46, 2.2, 0.114, 7.0, 20.0, 0.9964700000000001, 3.32, 0.63, 10.5], [6.3, 0.6, 0.1, 1.6, 0.048, 12.0, 26.0, 0.99306, 3.55, 0.51, 12.1], [7.2, 0.695, 0.13, 2.0, 0.076, 12.0, 20.0, 0.99546, 3.29, 0.54, 10.1], [6.6, 0.725, 0.2, 7.8, 0.073, 29.0, 79.0, 0.9977, 3.29, 0.54, 9.2], [7.8, 0.6, 0.14, 2.4, 0.086, 3.0, 15.0, 0.9975, 3.42, 0.6, 10.8], [8.6, 0.49, 0.28, 1.9, 0.11, 20.0, 136.0, 0.9972, 2.93, 1.95, 9.9], [8.3, 0.54, 0.28, 1.9, 0.077, 11.0, 40.0, 0.9978, 3.39, 0.61, 10.0], [8.2, 0.4, 0.44, 2.8, 0.08900000000000001, 11.0, 43.0, 0.9975, 3.53, 0.61, 10.5], [6.4, 0.37, 0.25, 1.9, 0.07400000000000001, 21.0, 49.0, 0.9974, 3.57, 0.62, 9.8], [8.9, 0.59, 0.5, 2.0, 0.337, 27.0, 81.0, 0.9964, 3.04, 1.61, 9.5], [12.0, 0.38, 0.56, 2.1, 0.09300000000000001, 6.0, 24.0, 0.99925, 3.14, 0.71, 10.9], [7.9, 0.545, 0.06, 4.0, 0.087, 27.0, 61.0, 0.9965, 3.36, 0.67, 10.7], [13.3, 0.34, 0.52, 3.2, 0.094, 17.0, 53.0, 1.0014, 3.05, 0.81, 9.5], [7.1, 0.35, 0.29, 2.5, 0.096, 20.0, 53.0, 0.9962, 3.42, 0.65, 11.0], [8.9, 0.43, 0.45, 1.9, 0.052000000000000005, 6.0, 16.0, 0.9948, 3.35, 0.7, 12.5], [10.7, 0.67, 0.22, 2.7, 0.107, 17.0, 34.0, 1.0004, 3.28, 0.98, 9.9], [11.5, 0.45, 0.5, 3.0, 0.078, 19.0, 47.0, 1.0003, 3.26, 1.11, 11.0], [7.8, 0.46, 0.26, 1.9, 0.08800000000000001, 23.0, 53.0, 0.9981, 3.43, 0.74, 9.2], [9.0, 0.43, 0.34, 2.5, 0.08, 26.0, 86.0, 0.9987, 3.38, 0.62, 9.5], [10.4, 0.41, 0.55, 3.2, 0.076, 22.0, 54.0, 0.9996, 3.15, 0.89, 9.9], [10.3, 0.5, 0.42, 2.0, 0.069, 21.0, 51.0, 0.9982, 3.16, 0.72, 11.5], [9.3, 0.39, 0.4, 2.6, 0.073, 10.0, 26.0, 0.9984, 3.34, 0.75, 10.2], [10.7, 0.4, 0.48, 2.1, 0.125, 15.0, 49.0, 0.998, 3.03, 0.81, 9.7], [8.1, 0.825, 0.24, 2.1, 0.084, 5.0, 13.0, 0.9972, 3.37, 0.77, 10.7], [9.5, 0.46, 0.24, 2.7, 0.092, 14.0, 44.0, 0.998, 3.12, 0.74, 10.0], [11.5, 0.35, 0.49, 3.3, 0.07, 10.0, 37.0, 1.0003, 3.32, 0.91, 11.0], [12.7, 0.59, 0.45, 2.3, 0.08199999999999999, 11.0, 22.0, 1.0, 3.0, 0.7, 9.3], [11.5, 0.31, 0.51, 2.2, 0.079, 14.0, 28.0, 0.9982, 3.03, 0.93, 9.8], [7.2, 0.57, 0.06, 1.6, 0.076, 9.0, 27.0, 0.9972, 3.36, 0.7, 9.6], [7.0, 0.65, 0.02, 2.1, 0.066, 8.0, 25.0, 0.9972, 3.47, 0.67, 9.5], [8.2, 0.59, 0.0, 2.5, 0.09300000000000001, 19.0, 58.0, 1.0002, 3.5, 0.65, 9.3], [9.1, 0.68, 0.11, 2.8, 0.09300000000000001, 11.0, 44.0, 0.9988799999999999, 3.31, 0.55, 9.5], [10.0, 0.56, 0.24, 2.2, 0.079, 19.0, 58.0, 0.9991, 3.18, 0.56, 10.1], [12.6, 0.41, 0.54, 2.8, 0.10300000000000001, 19.0, 41.0, 0.99939, 3.21, 0.76, 11.3], [9.3, 0.36, 0.39, 1.5, 0.08, 41.0, 55.0, 0.9965200000000001, 3.47, 0.73, 10.9], [7.6, 0.715, 0.0, 2.1, 0.068, 30.0, 35.0, 0.9953299999999999, 3.48, 0.65, 11.4], [7.4, 0.52, 0.13, 2.4, 0.078, 34.0, 61.0, 0.9952799999999999, 3.43, 0.59, 10.8], [8.4, 0.36, 0.32, 2.2, 0.081, 32.0, 79.0, 0.9964, 3.3, 0.72, 11.0], [8.5, 0.47, 0.27, 1.9, 0.057999999999999996, 18.0, 38.0, 0.99518, 3.16, 0.85, 11.1], [7.4, 0.58, 0.0, 2.0, 0.064, 7.0, 11.0, 0.9956200000000001, 3.45, 0.58, 11.3], [8.0, 0.18, 0.37, 0.9, 0.049, 36.0, 109.0, 0.9900700000000001, 2.89, 0.44, 12.7], [8.9, 0.5, 0.21, 2.2, 0.08800000000000001, 21.0, 39.0, 0.99692, 3.33, 0.83, 11.1], [11.6, 0.23, 0.57, 1.8, 0.07400000000000001, 3.0, 8.0, 0.9981, 3.14, 0.7, 9.9], [8.7, 0.42, 0.45, 2.4, 0.07200000000000001, 32.0, 59.0, 0.9961700000000001, 3.33, 0.77, 12.0], [7.4, 0.49, 0.27, 2.1, 0.071, 14.0, 25.0, 0.9938799999999999, 3.35, 0.63, 12.0], [7.0, 0.69, 0.07, 2.5, 0.091, 15.0, 21.0, 0.99572, 3.38, 0.6, 11.3], [10.4, 0.52, 0.45, 2.0, 0.08, 6.0, 13.0, 0.99774, 3.22, 0.76, 11.4], [10.0, 0.35, 0.47, 2.0, 0.061, 6.0, 11.0, 0.99585, 3.23, 0.52, 12.0], [9.7, 0.42, 0.46, 2.1, 0.07400000000000001, 5.0, 16.0, 0.99649, 3.27, 0.74, 12.3], [7.0, 0.745, 0.12, 1.8, 0.114, 15.0, 64.0, 0.9958799999999999, 3.22, 0.59, 9.5], [10.2, 0.33, 0.46, 1.9, 0.081, 6.0, 9.0, 0.9962799999999999, 3.1, 0.48, 10.4], [12.6, 0.39, 0.49, 2.5, 0.08, 8.0, 20.0, 0.9992, 3.07, 0.82, 10.3], [7.0, 0.58, 0.28, 4.8, 0.085, 12.0, 69.0, 0.9963299999999999, 3.32, 0.7, 11.0], [7.8, 0.58, 0.13, 2.1, 0.102, 17.0, 36.0, 0.9944, 3.24, 0.53, 11.2], [5.9, 0.395, 0.13, 2.4, 0.055999999999999994, 14.0, 28.0, 0.9936200000000001, 3.62, 0.67, 12.4], [7.5, 0.63, 0.27, 2.0, 0.083, 17.0, 91.0, 0.99616, 3.26, 0.58, 9.8], [7.4, 0.6, 0.26, 2.1, 0.083, 17.0, 91.0, 0.99616, 3.29, 0.56, 9.8], [7.2, 0.62, 0.01, 2.3, 0.065, 8.0, 46.0, 0.9933200000000001, 3.32, 0.51, 11.8], [7.5, 0.57, 0.02, 2.6, 0.077, 11.0, 35.0, 0.9955700000000001, 3.36, 0.62, 10.8], [6.6, 0.96, 0.0, 1.8, 0.08199999999999999, 5.0, 16.0, 0.9936, 3.5, 0.44, 11.9], [10.2, 0.54, 0.37, 15.4, 0.214, 55.0, 95.0, 1.00369, 3.18, 0.77, 9.0], [6.9, 0.63, 0.01, 2.4, 0.076, 14.0, 39.0, 0.9952200000000001, 3.34, 0.53, 10.8], [6.8, 0.81, 0.05, 2.0, 0.07, 6.0, 14.0, 0.9956200000000001, 3.51, 0.66, 10.8], [6.0, 0.42, 0.19, 2.0, 0.075, 22.0, 47.0, 0.9952200000000001, 3.39, 0.78, 10.0], [6.2, 0.52, 0.08, 4.4, 0.071, 11.0, 32.0, 0.99646, 3.56, 0.63, 11.6], [6.4, 0.36, 0.53, 2.2, 0.23, 19.0, 35.0, 0.9934, 3.37, 0.93, 12.4], [7.5, 0.31, 0.41, 2.4, 0.065, 34.0, 60.0, 0.99492, 3.34, 0.85, 11.4], [12.8, 0.3, 0.74, 2.6, 0.095, 9.0, 28.0, 0.9994, 3.2, 0.77, 10.8], [8.9, 0.4, 0.32, 5.6, 0.087, 10.0, 47.0, 0.9991, 3.38, 0.77, 10.5], [10.5, 0.42, 0.66, 2.95, 0.11599999999999999, 12.0, 29.0, 0.997, 3.24, 0.75, 11.7], [9.9, 0.4, 0.53, 6.7, 0.09699999999999999, 6.0, 19.0, 0.9986, 3.27, 0.82, 11.7], [10.4, 0.33, 0.63, 2.8, 0.084, 5.0, 22.0, 0.9998, 3.26, 0.74, 11.2], [13.3, 0.29, 0.75, 2.8, 0.084, 23.0, 43.0, 0.9986, 3.04, 0.68, 11.4], [7.8, 0.64, 0.1, 6.0, 0.115, 5.0, 11.0, 0.9984, 3.37, 0.69, 10.1], [6.7, 0.28, 0.28, 2.4, 0.012, 36.0, 100.0, 0.99064, 3.26, 0.39, 11.7], [10.7, 0.52, 0.38, 2.6, 0.066, 29.0, 56.0, 0.99577, 3.15, 0.79, 12.1], [7.2, 0.38, 0.38, 2.8, 0.068, 23.0, 42.0, 0.99356, 3.34, 0.72, 12.9], [8.9, 0.12, 0.45, 1.8, 0.075, 10.0, 21.0, 0.9955200000000001, 3.41, 0.76, 11.9], [9.8, 0.34, 0.39, 1.4, 0.066, 3.0, 7.0, 0.9947, 3.19, 0.55, 11.4], [8.9, 0.35, 0.4, 3.6, 0.11, 12.0, 24.0, 0.99549, 3.23, 0.7, 12.0], [9.5, 0.39, 0.41, 8.9, 0.069, 18.0, 39.0, 0.99859, 3.29, 0.81, 10.9], [7.9, 0.3, 0.68, 8.3, 0.05, 37.5, 278.0, 0.99316, 3.01, 0.51, 12.3], [7.4, 0.36, 0.34, 1.8, 0.075, 18.0, 38.0, 0.9933, 3.38, 0.88, 13.6], [7.1, 0.66, 0.0, 2.4, 0.052000000000000005, 6.0, 11.0, 0.99318, 3.35, 0.66, 12.7], [9.1, 0.36, 0.39, 1.8, 0.06, 21.0, 55.0, 0.99495, 3.18, 0.82, 11.0], [7.9, 0.2, 0.35, 1.7, 0.054000000000000006, 7.0, 15.0, 0.9945799999999999, 3.32, 0.8, 11.9], [6.7, 0.32, 0.44, 2.4, 0.061, 24.0, 34.0, 0.99484, 3.29, 0.8, 11.6], [7.6, 0.68, 0.02, 1.3, 0.07200000000000001, 9.0, 20.0, 0.9965, 3.17, 1.08, 9.2], [8.2, 0.915, 0.27, 2.1, 0.08800000000000001, 7.0, 23.0, 0.9962, 3.26, 0.47, 10.0], [7.3, 0.35, 0.24, 2.0, 0.067, 28.0, 48.0, 0.9957600000000001, 3.43, 0.54, 10.0], [6.5, 0.67, 0.0, 4.3, 0.057, 11.0, 20.0, 0.9948799999999999, 3.45, 0.56, 11.8], [6.7, 1.04, 0.08, 2.3, 0.067, 19.0, 32.0, 0.9964799999999999, 3.52, 0.57, 11.0], [5.0, 0.42, 0.24, 2.0, 0.06, 19.0, 50.0, 0.9917, 3.72, 0.74, 14.0], [7.1, 0.875, 0.05, 5.7, 0.08199999999999999, 3.0, 14.0, 0.99808, 3.4, 0.52, 10.2]]]\n",
      "[None, None, None, None, None, None, None, None, None, None]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'numCorrectPredictions = 0\\nnumWrongPredictions = 0\\nfor i,fold in enumerate(X_test):\\n    for index in range(len(fold)):\\n        mpg_values = [predicted_values[i][index], y_test[i][index]] #predicted/actual\\n        #print(mpg_values)\\n        #Decide rankings for predicted/actual values\\n        mpg_values = myutils.determine_MPG_rankings(mpg_values)\\n        #print(mpg_values)\\n        if(mpg_values[0]==mpg_values[1]):\\n            numCorrectPredictions = numCorrectPredictions+1\\n        else:\\n            numWrongPredictions = numWrongPredictions+1\\n\\naccuracy = np.round((numCorrectPredictions)/(numCorrectPredictions+numWrongPredictions),3)\\nerror_rate = np.round((numWrongPredictions)/(numCorrectPredictions+numWrongPredictions),3)\\n\\nprint(\"Decision Tree: accuracy = {}, error rate = {}\".format(accuracy,error_rate))'"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "importlib.reload(mysklearn.mypytable)\n",
    "importlib.reload(mysklearn.myevaluation)\n",
    "import numpy as np\n",
    "\n",
    "# Get data from csv file\n",
    "table = MyPyTable().load_from_file(os.path.join(\"input_files\",\"winequality-red.csv\"))\n",
    "y_col = table.get_column(\"quality\", False)\n",
    "x_cols = table.drop_col(\"quality\")\n",
    "\n",
    "# TODO: NEED TO CHANGE X-COLS TO CATEGORICAL ATTRIBUTES\n",
    "\n",
    "# Use Decision Tree to classify\n",
    "testcase = MyDecisionTreeClassifier()\n",
    "\n",
    "#Returns x INDEXES\n",
    "X_train, X_test = myevaluation.stratified_kfold_cross_validation(x_cols,y_col,n_splits=10)\n",
    "X_train, X_test, y_train, y_test = myutils.getInstances(X_train, X_test, x_cols,y_col)\n",
    "\n",
    "predicted_values = []\n",
    "for i,fold in enumerate(X_train):\n",
    "    #train,test = myutils.normalize_values(X_train[i],X_test[i])\n",
    "    train = X_train[i]\n",
    "    test = X_test[i]\n",
    "    testcase.fit(train,y_train[i])\n",
    "    predicted_values.append(testcase.predict(test))\n",
    "\n",
    "print(predicted_values)\n",
    "\n",
    "#testcase.print_decision_rules(table.column_names[:-1],\"quality\")\n",
    "\n",
    "'''numCorrectPredictions = 0\n",
    "numWrongPredictions = 0\n",
    "for i,fold in enumerate(X_test):\n",
    "    for index in range(len(fold)):\n",
    "        mpg_values = [predicted_values[i][index], y_test[i][index]] #predicted/actual\n",
    "        #print(mpg_values)\n",
    "        #Decide rankings for predicted/actual values\n",
    "        mpg_values = myutils.determine_MPG_rankings(mpg_values)\n",
    "        #print(mpg_values)\n",
    "        if(mpg_values[0]==mpg_values[1]):\n",
    "            numCorrectPredictions = numCorrectPredictions+1\n",
    "        else:\n",
    "            numWrongPredictions = numWrongPredictions+1\n",
    "\n",
    "accuracy = np.round((numCorrectPredictions)/(numCorrectPredictions+numWrongPredictions),3)\n",
    "error_rate = np.round((numWrongPredictions)/(numCorrectPredictions+numWrongPredictions),3)\n",
    "\n",
    "print(\"Decision Tree: accuracy = {}, error rate = {}\".format(accuracy,error_rate))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}